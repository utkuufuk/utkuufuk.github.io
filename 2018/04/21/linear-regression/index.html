<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/fav.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/fav.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/fav.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="5vPHUo26Cu0m53rV_72s-HEXqUFOM-Wcc-eI-CrKntk">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Inter:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"utkuufuk.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"always","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":false,"async":true,"transition":{"post_header":"slideDownIn","post_body":"slideDownIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Hey everyone, welcome to my first blog post! This is going to be a walkthrough on training a simple linear regression model in Python. I’ll show you how to do it from scratch, without using any machin">
<meta property="og:type" content="article">
<meta property="og:title" content="Training a Simple Linear Regression Model From Scratch">
<meta property="og:url" content="https://utkuufuk.com/2018/04/21/linear-regression/">
<meta property="og:site_name" content="Utku&#39;s Blog">
<meta property="og:description" content="Hey everyone, welcome to my first blog post! This is going to be a walkthrough on training a simple linear regression model in Python. I’ll show you how to do it from scratch, without using any machin">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://utkuufuk.com/2018/04/21/linear-regression/linear-regression_3_0.png">
<meta property="og:image" content="https://utkuufuk.com/2018/04/21/linear-regression/intercept_term.png">
<meta property="og:image" content="https://utkuufuk.com/2018/04/21/linear-regression/linear-regression_15_0.png">
<meta property="og:image" content="https://utkuufuk.com/2018/04/21/linear-regression/linear-regression_19_0.png">
<meta property="og:image" content="https://utkuufuk.com/2018/04/21/linear-regression/linear-regression_21_0.png">
<meta property="article:published_time" content="2018-04-21T11:20:15.000Z">
<meta property="article:modified_time" content="2025-02-11T09:13:35.295Z">
<meta property="article:author" content="Utku Ufuk">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://utkuufuk.com/2018/04/21/linear-regression/linear-regression_3_0.png">

<link rel="canonical" href="https://utkuufuk.com/2018/04/21/linear-regression/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Training a Simple Linear Regression Model From Scratch | Utku's Blog</title>
  
    <script>
      function sendPageView() {
        if (CONFIG.hostname !== location.hostname) return;
        var uid = localStorage.getItem('uid') || (Math.random() + '.' + Math.random());
        localStorage.setItem('uid', uid);
        navigator.sendBeacon('https://www.google-analytics.com/collect', new URLSearchParams({
          v  : 1,
          tid: 'UA-117512202-1',
          cid: uid,
          t  : 'pageview',
          dp : encodeURIComponent(location.pathname)
        }));
      }
      document.addEventListener('pjax:complete', sendPageView);
      sendPageView();
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Utku's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Utku's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-subscribe">

    <a href="/subscribe" rel="section"><i class="fas fa-bell fa-fw"></i>Subscribe</a>

  </li>
        <li class="menu-item menu-item-rss">

    <a href="/atom.xml" rel="section"><i class="fas fa-rss fa-fw"></i>RSS</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://utkuufuk.com/2018/04/21/linear-regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Utku Ufuk">
      <meta itemprop="description" content="Software Engineer">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Utku's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Training a Simple Linear Regression Model From Scratch
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-21 14:20:15" itemprop="dateCreated datePublished" datetime="2018-04-21T14:20:15+03:00">2018-04-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Hey everyone, welcome to my first blog post! This is going to be a walkthrough on training a simple linear regression model in Python. I’ll show you how to do it from scratch, without using any machine learning tools or libraries. We’ll only use <a target="_blank" rel="noopener" href="http://www.numpy.org/">NumPy</a> and <a target="_blank" rel="noopener" href="https://matplotlib.org/">Matplotlib</a> for matrix operations and data visualization.</p>
<a id="more"></a>

<h4 id="Problem-amp-Dataset"><a href="#Problem-amp-Dataset" class="headerlink" title="Problem &amp; Dataset"></a>Problem &amp; Dataset</h4><p>We’ll look at a regression problem from a very popular <a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning">machine learning course</a> taught by <a target="_blank" rel="noopener" href="http://www.andrewng.org/">Andrew Ng</a>. Our objective in this problem will be to train a model that accurately predicts the profits of a food truck.</p>
<p>The first column in our <a href="/2018/04/21/linear-regression/food_truck_data.txt" title="dataset file">dataset file</a> contains city populations and the second column contains food truck profits in each city, both in $10,000$s. Here are the first few training examples:</p>
<figure class="highlight sh"><figcaption><span>food_truck_data.txt</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">6.1101,17.592</span><br><span class="line">5.5277,9.1302</span><br><span class="line">8.5186,13.662</span><br><span class="line">7.0032,11.854</span><br><span class="line">5.8598,6.8233</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>We’re going to use this dataset as a training sample to build our model. Let’s begin by loading it:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data = np.loadtxt(<span class="string">&#x27;food_truck_data.txt&#x27;</span>, delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line">x = data[:, <span class="number">0</span>] <span class="comment"># city populations</span></span><br><span class="line">y = data[:, <span class="number">1</span>] <span class="comment"># food truck profits</span></span><br></pre></td></tr></table></figure>
<p>Both $x$ and $y$ are one dimensional arrays, because we have one <strong>feature</strong> (population) and one <strong>target variable</strong> (profit) in this problem. Therefore we can conveniently visualize our dataset using a scatter plot:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.scatter(x, y, marker=<span class="string">&quot;x&quot;</span>, c=<span class="string">&quot;red&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Food Truck Dataset&quot;</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;City Population in 10,000s&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Food Truck Profit in 10,000s&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">4</span>, <span class="number">25</span>, -<span class="number">5</span>, <span class="number">25</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="/2018/04/21/linear-regression/linear-regression_3_0.png" class="">


<h4 id="Hypothesis-Function"><a href="#Hypothesis-Function" class="headerlink" title="Hypothesis Function"></a>Hypothesis Function</h4><p>Now we need to come up with a <strong>straight line</strong> which accurately represents the relationship between population and profit. This is called the <strong>hypothesis function</strong> and it’s formulated as:</p>
<p>$h_\theta(x) = \theta^Tx = \theta_0 + \theta_1x_1 + \theta_2x_2 + … + \theta_nx_n$</p>
<p>where $x$ corresponds to the feature matrix and $\theta$ corresponds to the vector of <strong>model parameters.</strong></p>
<p>Since we have a single feature $x_1,$ we’ll only have two model parameters $\theta_0$ and $\theta_1$ in our hypothesis function:</p>
<p>$h_\theta(x) = \theta_0 + \theta_1x_1$</p>
<p>As you may have noticed, the number of model parameters is equal to the number of features plus $1$. That’s because each feature is weighted by a parameter to control its impact on the hypothesis $h_\theta(x)$. There is also an independent parameter $\theta_0$ called the <strong>intercept term,</strong> which defines the point where the hypothesis function intercepts the $y$-axis as demonstrated below:</p>
<img src="/2018/04/21/linear-regression/intercept_term.png" class="">
<br>
The predictions of a hypothesis function can easily be evaluated in Python by computing the cross product of $x$ and $\theta^T.$ At the moment we have our $x$ and $y$ vectors but we don't have our model parameters yet. So let's create those as well and initialize them with zeros:

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta = np.zeros(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>Also, we have to make sure that the matrix dimensions of $x$ and $\theta^T$ are compatible with each other for cross product. Currently $x$ has $1$ column but $\theta^T$ has $2$ rows. The dimensions don’t match because of the additional intercept term $\theta_0.$</p>
<p>We can solve this issue by prepending a column to $x$ and set it to all ones. This is essentially equivalent to creating a new feature $x_0 = 1.$ This extra column won’t affect the hypothesis whatsoever because $\theta_0$ is going to be multiplied by $1$ in the cross product.</p>
<p>Let’s create a new variable $X$ to store the extended $x$ matrix:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = np.ones(shape=(<span class="built_in">len</span>(x), <span class="number">2</span>))</span><br><span class="line">X[:, <span class="number">1</span>] = x</span><br></pre></td></tr></table></figure>
<p>Finally, we can compute the predictions of our hypothesis as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions = X @ theta</span><br></pre></td></tr></table></figure>
<p>Of course, the predictions are currently all zeros because we haven’t trained our model yet.</p>
<h4 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h4><p>The objective in training a linear regression model is to minimize a <strong>cost function</strong>, which measures the difference between actual $y$ values in the training sample and predictions made by the hypothesis function $h_\theta(x)$.</p>
<p>Such a cost function can be formulated as;</p>
<p>$J(\theta) = \dfrac{1}{2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2$</p>
<p>where $m$ is the number of training examples.</p>
<p>Here’s its Python version:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">theta, X, y</span>):</span></span><br><span class="line">    predictions = X @ theta</span><br><span class="line">    squared_errors = np.square(predictions - y)</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(squared_errors) / (<span class="number">2</span> * <span class="built_in">len</span>(y))</span><br></pre></td></tr></table></figure>
<p>Now let’s take a look at the cost of our initial untrained model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;The initial cost is:&#x27;</span>, cost(theta, X, y))</span><br></pre></td></tr></table></figure>
<pre><code>The initial cost is: 32.0727338775</code></pre>
<h4 id="Gradient-Descent-Algorithm"><a href="#Gradient-Descent-Algorithm" class="headerlink" title="Gradient Descent Algorithm"></a>Gradient Descent Algorithm</h4><p>Since our hypothesis is based on the model parameters $\theta$, we must somehow adjust them to minimize our cost function $J(\theta)$. This is where the <strong>gradient descent</strong> algorithm comes into play. It’s an optimization algorithm which can be used in minimizing <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Differentiable_function">differentiable</a> functions. Luckily our cost function $J(\theta)$ happens to be a differentiable one.</p>
<p>So here’s how the gradient descent algorithm works in a nutshell: </p>
<p>In each iteration, it takes a small step in the opposite gradient direction of $J(\theta)$. This makes the model parameters $\theta$ gradually come closer to the optimal values. This process is repeated until eventually the minimum cost is achieved.</p>
<p>More formally, gradient descent performs the following update in each iteration: </p>
<p>$\theta_j := \theta_j - \alpha\frac{1}{m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j$</p>
<p>The $\alpha$ term here is called the <strong>learning rate</strong>. It allows us to control the step size to update $\theta$ in each iteration. Choosing a too large learning rate may prevent us from converging to a minimum cost, whereas choosing a too small learning rate may significantly slow down the algorithm.</p>
<p>Here’s a generic implementation of the gradient descent algorithm:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">X, y, alpha, num_iters</span>):</span></span><br><span class="line">    num_features = X.shape[<span class="number">1</span>]               </span><br><span class="line">    theta = np.zeros(num_features)          <span class="comment"># initialize model parameters</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        predictions = X @ theta             <span class="comment"># compute predictions based on the current hypothesis</span></span><br><span class="line">        errors = predictions - y</span><br><span class="line">        gradient = X.transpose() @ errors</span><br><span class="line">        theta -= alpha * gradient / <span class="built_in">len</span>(y)  <span class="comment"># update model parameters</span></span><br><span class="line">    <span class="keyword">return</span> theta                            <span class="comment"># return optimized parameters</span></span><br></pre></td></tr></table></figure>
<p>Now let’s use this function to train our model and plot the hypothesis function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">theta = gradient_descent(X, y, <span class="number">0.02</span>, <span class="number">600</span>)   <span class="comment"># run GD for 600 iterations with learning rate = 0.02</span></span><br><span class="line">predictions = X @ theta                     <span class="comment"># predictions made by the optimized model</span></span><br><span class="line">ax.plot(X[:, <span class="number">1</span>], predictions, linewidth=<span class="number">2</span>)  <span class="comment"># plot the hypothesis on top of the training data</span></span><br><span class="line">fig</span><br></pre></td></tr></table></figure>
<img src="/2018/04/21/linear-regression/linear-regression_15_0.png" class="">

<h4 id="Debugging"><a href="#Debugging" class="headerlink" title="Debugging"></a>Debugging</h4><p>Our linear fit looks pretty good, right? The algorithm must have successfully optimized our model.</p>
<p>Well, to be honest, it’s been fairly easy to visualize the hypothesis because there’s only one feature in this problem. </p>
<p>But what if we had multiple features? Then it wouldn’t be possible to simply plot the hypothesis to see whether the algorithm has worked as intended or not.</p>
<p>Fortunately, there’s a simple way to debug the gradient descent algorithm irrespective of the number of features:</p>
<ol>
<li>Modify the gradient descent function to make it record the cost at the end of each iteration. </li>
<li>Plot the cost history after the gradient descent has finished.</li>
<li>Pat yourself on the back if you see that the cost has monotonically decreased over time.</li>
</ol>
<p>Here’s the modified version of our gradient descent function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">X, y, alpha, num_iters</span>):</span></span><br><span class="line">    cost_history = np.zeros(num_iters)          <span class="comment"># create a vector to store the cost history</span></span><br><span class="line">    num_features = X.shape[<span class="number">1</span>]               </span><br><span class="line">    theta = np.zeros(num_features)</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        predictions = X @ theta</span><br><span class="line">        errors = predictions - y</span><br><span class="line">        gradient = X.transpose() @ errors</span><br><span class="line">        theta -= alpha * gradient / <span class="built_in">len</span>(y)</span><br><span class="line">        cost_history[n] = cost(theta, X, y)     <span class="comment"># compute and record the cost</span></span><br><span class="line">    <span class="keyword">return</span> theta, cost_history                  <span class="comment"># return optimized parameters and cost history</span></span><br></pre></td></tr></table></figure>
<p>Now let’s try learning rates $0.01$, $0.015$, $0.02$ and plot the cost history for each one:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">num_iters = <span class="number">1200</span></span><br><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.015</span>, <span class="number">0.02</span>]</span><br><span class="line"><span class="keyword">for</span> lr <span class="keyword">in</span> learning_rates:</span><br><span class="line">    _, cost_history = gradient_descent(X, y, lr, num_iters)</span><br><span class="line">    plt.plot(cost_history, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Gradient descent with different learning rates&quot;</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;number of iterations&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;cost&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.legend(<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">str</span>, learning_rates)))</span><br><span class="line">plt.axis([<span class="number">0</span>, num_iters, <span class="number">4</span>, <span class="number">6</span>])</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="/2018/04/21/linear-regression/linear-regression_19_0.png" class="" title="image_description">

<p>It appears that the gradient descent algorithm worked correctly for these particular learning rates. Notice that it takes more iterations to minimize the cost as the learning rate decreases.</p>
<p>Now let’s try a larger learning rate and see what happens:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.025</span></span><br><span class="line">num_iters = <span class="number">50</span></span><br><span class="line">_, cost_history = gradient_descent(X, y, learning_rate, num_iters)</span><br><span class="line">plt.plot(cost_history, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Gradient descent with learning rate = &quot;</span> + <span class="built_in">str</span>(learning_rate), fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;number of iterations&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;cost&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, num_iters, <span class="number">0</span>, <span class="number">6000</span>])</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="/2018/04/21/linear-regression/linear-regression_21_0.png" class="" title="image_description">
<br>
Doesn't look good... That's what happens when the learning rate is too large. Even though the gradient descent algorithm takes steps in the correct direction, these steps are so huge that it's going to overshoot the target and the cost diverges from the minimum value instead of converging to it.

<p>Right now we can safely set the learning rate to $0.02$, because it allows us to minimize the cost and it requires relatively fewer iterations to converge.</p>
<h4 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h4><p>Now that we’ve learned how to train our model, we can finally predict the food truck profit for a particular city:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">theta, _ = gradient_descent(X, y, <span class="number">0.02</span>, <span class="number">600</span>)    <span class="comment"># train the model</span></span><br><span class="line">test_example = np.array([<span class="number">1</span>, <span class="number">7</span>])                 <span class="comment"># pick a city with 70,000 population as a test example</span></span><br><span class="line">prediction = test_example @ theta               <span class="comment"># use the trained model to make a prediction</span></span><br><span class="line">print(<span class="string">&#x27;For population = 70,000, we predict a profit of $&#x27;</span>, prediction * <span class="number">10000</span>);</span><br></pre></td></tr></table></figure>
<pre><code>For population = 70,000, we predict a profit of $ 45905.6621788</code></pre>
<p>If you’re still here, you should <a href="/subscribe">subscribe</a> to get updates on my future articles.</p>

    </div>

    
    
    
        

  <div class="followme">
    <p>Follow me on</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://github.com/utkuufuk">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>

            <span class="label">GitHub</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://twitter.com/utkufu">
            <span class="icon">
              <i class="fab fa-twitter"></i>
            </span>

            <span class="label">Twitter</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://youtube.com/utkuufuk">
            <span class="icon">
              <i class="fab fa-youtube"></i>
            </span>

            <span class="label">YouTube</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/subscribe">
            <span class="icon">
              <i class="fas fa-bell"></i>
            </span>

            <span class="label">Subscribe</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"><i class="fa fa-tag"></i> Python</a>
              <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
              <a href="/tags/AI/" rel="tag"><i class="fa fa-tag"></i> AI</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2018/05/04/learning-curves/" rel="next" title="Learning Curves in Linear & Polynomial Regression">
      Learning Curves in Linear & Polynomial Regression <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Problem-amp-Dataset"><span class="nav-number">1.</span> <span class="nav-text">Problem &amp; Dataset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hypothesis-Function"><span class="nav-number">2.</span> <span class="nav-text">Hypothesis Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cost-Function"><span class="nav-number">3.</span> <span class="nav-text">Cost Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-Descent-Algorithm"><span class="nav-number">4.</span> <span class="nav-text">Gradient Descent Algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Debugging"><span class="nav-number">5.</span> <span class="nav-text">Debugging</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Prediction"><span class="nav-number">6.</span> <span class="nav-text">Prediction</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Utku Ufuk"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Utku Ufuk</p>
  <div class="site-description" itemprop="description">Software Engineer</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/utkuufuk" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;utkuufuk" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/utkufu" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;utkufu" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://youtube.com/utkuufuk" title="YouTube → https:&#x2F;&#x2F;youtube.com&#x2F;utkuufuk" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://linkedin.com/in/utku-ufuk" title="LinkedIn → https:&#x2F;&#x2F;linkedin.com&#x2F;in&#x2F;utku-ufuk" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:utkuufuk@gmail.com" title="E-Mail → mailto:utkuufuk@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="/atom.xml" title="&#x2F;atom.xml">RSS</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="/subscribe" title="&#x2F;subscribe">Subscribe</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Utku Ufuk</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
